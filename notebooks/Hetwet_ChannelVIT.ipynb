{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ChannelVIT to Predict Treatment and Outcome\n",
    "## GPU Accelerated\n",
    "**Date: March 5th, 2025**\n",
    "\n",
    "*Decription: This code read the three bands from the paths and the treatment/outcome value and return the torch dataset for further implementation in the ChannelViT. Then it splits the data into train and test set to do 20-80 cross-validation, and implement ChannelViT on them. It test set afterwards. Then it returns the predicted and the true values for further implementing in the R-Learner.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tifffile as tiff\n",
    "from skimage.transform import resize\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from channelvit import transformations\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from omegaconf import OmegaConf\n",
    "from channelvit.transformations.so2sat import So2SatAugmentation\n",
    "# Adding GPU Monitoring\n",
    "import sys\n",
    "sys.path.append('Scripts')  # Make sure the Scripts directory is in the path\n",
    "from gpu_monitor import get_gpu_usage, log_gpu_metrics, print_gpu_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataset Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBandGrayTIFFDataset(Dataset):\n",
    "    \"\"\"Dataset for multiband grayscale TIFF images with continuous treatment labels.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metadata_path: str,  # Path to the metadata file (CSV or Parquet)\n",
    "        transform_cfg: DictConfig,\n",
    "        is_train: bool = True,\n",
    "        channels: list = None,       # List of channel indices to use (e.g., [0, 1])\n",
    "        channel_mask: bool = False,  # Whether to mask unselected channels\n",
    "        scale: float = 1.0,\n",
    "        indices: list = None         # To split to train and test set\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load metadata\n",
    "        if metadata_path.endswith(\".parquet\"):\n",
    "            self.df = pd.read_parquet(metadata_path)\n",
    "        else:\n",
    "            self.df = pd.read_csv(metadata_path)\n",
    "\n",
    "        # Keep only indices that are \n",
    "        if indices is not None:\n",
    "            self.df = self.df.iloc[indices].reset_index(drop=True)\n",
    "        \n",
    "        # Convert channels to a tensor if provided\n",
    "        self.channels = torch.tensor(channels) if channels is not None else None\n",
    "        self.scale = scale\n",
    "        self.channel_mask = channel_mask\n",
    "\n",
    "        # Initialize transformation using ChannelViT's augmentation\n",
    "        self.transform = getattr(transformations, transform_cfg.name)(\n",
    "            is_train,\n",
    "            **transform_cfg.args,\n",
    "            normalization_mean=transform_cfg.normalization.mean,\n",
    "            normalization_std=transform_cfg.normalization.std,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Retrieve a sample, apply resizing, transformation, and return the image and label.\"\"\"\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        # Get file paths for the two TIFF images\n",
    "        img1_path = row[\"image1_path\"]  \n",
    "        img2_path = row[\"image2_path\"]\n",
    "        img3_path = row[\"image3_path\"]\n",
    "        img4_path = row[\"image4_path\"]\n",
    "        img5_path = row[\"image5_path\"]\n",
    "        img6_path = row[\"image6_path\"]\n",
    "\n",
    "        # Check that files exist\n",
    "        for img_path in [img1_path, img2_path, img3_path, img4_path, img5_path, img6_path]:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Missing image file for index {index}: {img_path}\")\n",
    "\n",
    "\n",
    "        # Read the TIFF images as float32 arrays\n",
    "        img1 = tiff.imread(img1_path).astype(\"float32\")\n",
    "        img2 = tiff.imread(img2_path).astype(\"float32\")\n",
    "        img3 = tiff.imread(img3_path).astype(\"float32\")\n",
    "        img4 = tiff.imread(img4_path).astype(\"float32\")\n",
    "        img5 = tiff.imread(img5_path).astype(\"float32\")\n",
    "        img6 = tiff.imread(img6_path).astype(\"float32\")\n",
    "\n",
    "        # Optionally, print original shapes for debugging\n",
    "        # print(f\"Original Shapes: img1={img1.shape}, img2={img2.shape}\")\n",
    "\n",
    "        # Define target shape for both images (height, width)\n",
    "        target_shape = (256, 256)\n",
    "        img1 = resize(img1, target_shape, anti_aliasing=True) if img1.shape != target_shape else img1\n",
    "        img2 = resize(img2, target_shape, anti_aliasing=True) if img2.shape != target_shape else img2\n",
    "        img3 = resize(img3, target_shape, anti_aliasing=True) if img3.shape != target_shape else img3\n",
    "        img4 = resize(img4, target_shape, anti_aliasing=True) if img4.shape != target_shape else img4\n",
    "        img5 = resize(img5, target_shape, anti_aliasing=True) if img5.shape != target_shape else img5\n",
    "        img6 = resize(img6, target_shape, anti_aliasing=True) if img6.shape != target_shape else img6\n",
    "\n",
    "\n",
    "        # Stack the three images into a multi-channel array: (3, 256, 256)\n",
    "        img_chw = np.stack([img1, img2, img3, img4, img5, img6], axis=0)\n",
    "\n",
    "        # Apply the ChannelViT transformation (e.g., normalization and augmentation)\n",
    "        img_chw = self.transform(img_chw)\n",
    "\n",
    "        # Apply scaling if needed\n",
    "        if self.scale != 1.0:\n",
    "            img_chw *= self.scale\n",
    "\n",
    "        # If specific channels are provided, either mask or select them\n",
    "        if self.channels is not None:\n",
    "            if self.channel_mask:\n",
    "                # Set unselected channels to zero\n",
    "                unselected = [c for c in range(img_chw.shape[0]) if c not in self.channels.numpy()]\n",
    "                img_chw[unselected] = 0\n",
    "            else:\n",
    "                # Select only the specified channels\n",
    "                img_chw = img_chw[self.channels.numpy()]\n",
    "\n",
    "        # Convert to a contiguous array and then to a Torch tensor\n",
    "        img_chw = np.ascontiguousarray(img_chw)\n",
    "        img_tensor = torch.tensor(img_chw).float()\n",
    "\n",
    "        # Extract the treatment label as a continuous value from the \"treatment\" column\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, {\"label\": label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the dataset using the CSV metadata file\n",
    "# dataset = MultiBandGrayTIFFDataset(\n",
    "#     metadata_path = \"/Users/sayedmorteza/ChannelViT/Hetwet/hetwet_metadata_3.csv\",\n",
    "#     transform_cfg = transform_cfg,\n",
    "#     is_train = True,\n",
    "#     channels = [0, 1, 2],      # Use both channels\n",
    "#     channel_mask = False    # Set to True to mask unselected channels\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChannelViT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified ChannelViT\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from channelvit.backbone.vit import Block\n",
    "from channelvit.utils import trunc_normal_\n",
    "\n",
    "\n",
    "class PatchEmbedPerChannel(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) * (img_size // patch_size) * in_chans\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv3d(\n",
    "            1,\n",
    "            embed_dim,\n",
    "            kernel_size=(1, patch_size, patch_size),\n",
    "            stride=(1, patch_size, patch_size),\n",
    "        )\n",
    "\n",
    "        # Parameter shape: (1, embed_dim, in_chans, 1, 1)\n",
    "        self.channel_embed = nn.parameter.Parameter(\n",
    "            torch.zeros(1, embed_dim, in_chans, 1, 1)\n",
    "        )\n",
    "        trunc_normal_(self.channel_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        # Use channel index provided in extra_tokens; default to 0 if not provided.\n",
    "        cur_channels = int(extra_tokens.get(\"channels\", torch.tensor([0], device=x.device))[0].item())\n",
    "\n",
    "        B, Cin, H, W = x.shape\n",
    "        # shared projection layer across channels; output shape: (B, embed_dim, Cin, H_out, W_out)\n",
    "        x = self.proj(x.unsqueeze(1))\n",
    "\n",
    "        # Get the offset for the selected channel; expected shape: (1, embed_dim, 1, 1)\n",
    "        offset = self.channel_embed[:, :, cur_channels, :, :]\n",
    "        # Simply add the offset; broadcasting will automatically repeat it along dimension 2 (and others)\n",
    "        x = x[:, :, cur_channels, :, :] + offset\n",
    "        # x = x + offset\n",
    "\n",
    "        # Prepare the output sequence: flatten spatial dimensions and transpose.\n",
    "        x = x.flatten(2)  # shape: (B, embed_dim, Cin*H_out*W_out)\n",
    "        x = x.transpose(1, 2)  # shape: (B, Cin*H_out*W_out, embed_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelVisionTransformer(nn.Module):\n",
    "    \"\"\"Channel Vision Transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=[224],\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_classes=0,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = self.out_dim = embed_dim\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "        self.patch_embed = PatchEmbedPerChannel(\n",
    "            img_size=img_size[0],\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.num_extra_tokens = 1  # cls token\n",
    "\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                1, num_patches // self.in_chans + self.num_extra_tokens, embed_dim\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[i],\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = (\n",
    "            nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h, c):\n",
    "        if not hasattr(self, \"num_extra_tokens\"):\n",
    "            num_extra_tokens = 1\n",
    "        else:\n",
    "            num_extra_tokens = self.num_extra_tokens\n",
    "\n",
    "        npatch = x.shape[1] - num_extra_tokens\n",
    "        N = self.pos_embed.shape[1] - num_extra_tokens\n",
    "\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "\n",
    "        class_pos_embed = self.pos_embed[:, :num_extra_tokens]\n",
    "        patch_pos_embed = self.pos_embed[:, num_extra_tokens:]\n",
    "\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(\n",
    "                1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
    "            ).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode=\"bicubic\",\n",
    "        )\n",
    "        assert (\n",
    "            int(w0) == patch_pos_embed.shape[-2]\n",
    "            and int(h0) == patch_pos_embed.shape[-1]\n",
    "        )\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, 1, -1, dim)\n",
    "        patch_pos_embed = patch_pos_embed.expand(1, c, -1, dim).reshape(1, -1, dim)\n",
    "\n",
    "        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x, extra_tokens):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x, extra_tokens)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h, nc)\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x, extra_tokens={}):\n",
    "        x = self.prepare_tokens(x, extra_tokens)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x, extra_tokens={}):\n",
    "        x = self.prepare_tokens(x, extra_tokens)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "    def get_intermediate_layers(self, x, extra_tokens={}, n=1):\n",
    "        x = self.prepare_tokens(x, extra_tokens)\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "def channelvit_tiny(patch_size=16, **kwargs):\n",
    "    model = ChannelVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=192,\n",
    "        depth=12,\n",
    "        num_heads=3,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def channelvit_small(patch_size=16, **kwargs):\n",
    "    model = ChannelVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def channelvit_base(patch_size=16, **kwargs):\n",
    "    model = ChannelVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the ChannelViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Global Training Setup\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Define the transformation configuration for data augmentation\n",
    "transform_cfg = OmegaConf.create({\n",
    "    \"name\": \"So2SatAugmentation\",  # ChannelViT augmentation for So2Sat-style data\n",
    "    \"args\": {},\n",
    "    \"normalization\": {\n",
    "        \"mean\": [7354, 18, 27665, 4.36, 150, 1.3], \n",
    "        \"std\": [70, 3, 2600, 19, 40, 1]\n",
    "    }\n",
    "})\n",
    "\n",
    "# Counting the length of the scenes\n",
    "metadata_path = \"/work/10297/sm_malaekeh/ls6/Data/Hetwet/Controls/hetwet_metadata_6bands_treatment.csv\"\n",
    "full_df = pd.read_csv(metadata_path) if metadata_path.endswith(\".csv\") else pd.read_parquet(metadata_path)\n",
    "num_samples = len(full_df)\n",
    "all_indices = np.arange(num_samples); del full_df\n",
    "\n",
    "# Define model parameters\n",
    "img_size = [256]\n",
    "in_chans = 6\n",
    "patch_size = 16\n",
    "embed_dim = 768 # Increase\n",
    "depth = 12\n",
    "num_heads = 8 # Increase to 4 ~ 8\n",
    "num_classes = 1\n",
    "\n",
    "# Selecting the Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"---------------------\\nThe Device is {device}\\n------------------\")\n",
    "\n",
    "# 5-Fold Cross-Validation \n",
    "number_split = 5\n",
    "kf = KFold(n_splits = number_split, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, classifier, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model and classifier for one epoch and return the average loss.\n",
    "    \n",
    "    Args:\n",
    "        model: The main model (e.g., ChannelVisionTransformer).\n",
    "        classifier: The classifier head appended to the model.\n",
    "        dataloader: DataLoader providing training batches.\n",
    "        optimizer: Optimizer for updating model parameters.\n",
    "        criterion: Loss function (e.g., MSELoss).\n",
    "        device: Device (CPU or GPU) on which computations are performed.\n",
    "    \n",
    "    Returns:\n",
    "        average_loss: The average training loss over all batches.\n",
    "    \"\"\"\n",
    "    model.train()         # Set model to training mode\n",
    "    classifier.train()    # Set classifier to training mode\n",
    "    running_loss = 0.0    # Initialize cumulative loss\n",
    "    \n",
    "    # Loop over batches\n",
    "    for i, (imgs, metadata) in enumerate(dataloader, 1):\n",
    "        imgs = imgs.to(device)  # Move image batch to the device\n",
    "        \n",
    "        # Retrieve labels and ensure they have the correct shape (batch_size x 1)\n",
    "        labels = metadata[\"label\"].to(device).float().unsqueeze(1)\n",
    "        \n",
    "        # Example: Handling extra tokens for the model, if needed (adjust as necessary)\n",
    "        num_channels = imgs.shape[1]\n",
    "        cur_channels = min(num_channels - 1, model.in_chans - 1)\n",
    "        extra_tokens = {\"channels\": torch.tensor([cur_channels], dtype=torch.long, device=device)}\n",
    "        \n",
    "        optimizer.zero_grad()   # Zero the gradients before backward pass\n",
    "        \n",
    "        # Forward pass: compute model features and classifier predictions\n",
    "        features = model(imgs, extra_tokens)\n",
    "        predictions = classifier(features)\n",
    "        \n",
    "        # Compute the loss between predictions and true labels\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass: compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "        \n",
    "        # Print batch loss every 10 batches for monitoring\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Train Batch {i}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, classifier, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model and classifier on the validation/test set.\n",
    "    \n",
    "    Args:\n",
    "        model: The main model.\n",
    "        classifier: The classifier head.\n",
    "        dataloader: DataLoader providing validation/test batches.\n",
    "        criterion: Loss function.\n",
    "        device: Device (CPU or GPU) on which computations are performed.\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss over the evaluation set.\n",
    "        r2: R-squared score for predictions.\n",
    "        rmse: Root Mean Squared Error.\n",
    "        bias: Mean error (average residual: prediction - actual).\n",
    "        all_predictions: Flattened array of all predictions.\n",
    "        all_actuals: Flattened array of all actual label values.\n",
    "    \"\"\"\n",
    "    model.eval()         # Set model to evaluation mode\n",
    "    classifier.eval()     # Set classifier to evaluation mode\n",
    "    total_loss = 0.0      # Initialize cumulative loss\n",
    "    all_predictions = []  # List to store predictions from each batch\n",
    "    all_actuals = []      # List to store true labels from each batch\n",
    "    \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for imgs, metadata in dataloader:\n",
    "            imgs = imgs.to(device)  # Move images to the device\n",
    "            \n",
    "            # Retrieve labels and adjust their shape to (batch_size x 1)\n",
    "            labels = metadata[\"label\"].to(device).float().unsqueeze(1)\n",
    "            \n",
    "            # Handle extra tokens if required by the model architecture\n",
    "            num_channels = imgs.shape[1]\n",
    "            cur_channels = min(num_channels - 1, model.in_chans - 1)\n",
    "            extra_tokens = {\"channels\": torch.tensor([cur_channels], dtype=torch.long, device=device)}\n",
    "            \n",
    "            # Forward pass to get model predictions\n",
    "            features = model(imgs, extra_tokens)\n",
    "            predictions = classifier(features)\n",
    "            \n",
    "            # Calculate the loss for the batch\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Save predictions and actual labels for later metric calculations\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_actuals.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute the average loss over the evaluation set\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Stack and flatten the predictions and actual labels arrays\n",
    "    all_predictions = np.vstack(all_predictions).flatten()\n",
    "    all_actuals = np.vstack(all_actuals).flatten()\n",
    "    \n",
    "    # Compute regression metrics: R², RMSE, and Bias (mean error)\n",
    "    r2 = r2_score(all_actuals, all_predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(all_actuals, all_predictions))\n",
    "    bias = np.mean(all_predictions - all_actuals)\n",
    "    \n",
    "    return avg_loss, r2, rmse, bias, all_predictions, all_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "# # Redirect stderr to null to suppress error messages\n",
    "sys.stderr = open('/dev/null', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containers for predictions and learning curves\n",
    "results_list = []\n",
    "fold_r2_history = {}\n",
    "fold_bias_history = {}\n",
    "\n",
    "def run_epoch(\n",
    "    fold_num: int,\n",
    "    epoch: int,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model: nn.Module,\n",
    "    classifier: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    Run one full epoch of training and validation for a given fold.\n",
    "\n",
    "    Args:\n",
    "        fold_num (int): The current fold number in cross-validation (1-based).\n",
    "        epoch (int): The current epoch number (1-based).\n",
    "        train_loader (DataLoader): Dataloader providing batches for the training set.\n",
    "        val_loader (DataLoader): Dataloader providing batches for the validation set.\n",
    "        model (nn.Module): The primary model (e.g., a Vision Transformer).\n",
    "        classifier (nn.Module): A classifier head or final layer appended to the model.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        criterion (nn.Module): Loss function (e.g., MSELoss) used to compute training/validation loss.\n",
    "        device (torch.device): The device (CPU or GPU) on which computations will run.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            A 7-element tuple containing:\n",
    "            - train_loss (float): Average loss over all training batches for this epoch.\n",
    "            - val_loss (float): Average loss over all validation batches for this epoch.\n",
    "            - r2 (float): R-squared score computed on the validation set.\n",
    "            - rmse (float): Root Mean Squared Error computed on the validation set.\n",
    "            - bias (float): Mean error (prediction - actual) on the validation set.\n",
    "            - val_predictions (numpy.ndarray): Flattened array of predicted values for the validation set.\n",
    "            - val_actuals (numpy.ndarray): Flattened array of actual values for the validation set.\n",
    "\n",
    "    This function prints progress logs, including the training loss,\n",
    "    validation loss, R², RMSE, and bias, which helps in monitoring\n",
    "    model performance over epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print current fold and epoch for clarity\n",
    "    print(f\"\\nFold {fold_num}, Epoch {epoch}\")\n",
    "\n",
    "    # Train the model for one epoch (using a custom 'trainer' function)\n",
    "    train_loss = trainer(model, classifier, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    # Log GPU state after training\n",
    "    train_gpu = get_gpu_usage()\n",
    "    if train_gpu:\n",
    "        print(f\"\\nGPU State after training (Epoch {epoch}):\")\n",
    "        print_gpu_metrics(train_gpu)\n",
    "        log_gpu_metrics(train_gpu, \"gpu_usage.log\", epoch=epoch, batch='train')\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, r2, rmse, bias, val_predictions, val_actuals = evaluate(\n",
    "        model, classifier, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    # Log GPU state after validation\n",
    "    val_gpu = get_gpu_usage()\n",
    "    if val_gpu:\n",
    "        print(f\"\\nGPU State after validation (Epoch {epoch}):\")\n",
    "        print_gpu_metrics(val_gpu)\n",
    "        log_gpu_metrics(val_gpu, \"gpu_usage.log\", epoch=epoch, batch='val')\n",
    "\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(\n",
    "        f\"Fold {fold_num}, Epoch {epoch} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "        f\"R²: {r2:.3f} | RMSE: {rmse:.3f} | Bias: {bias:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Return a tuple of relevant metrics and arrays\n",
    "    return train_loss, val_loss, r2, rmse, bias, val_predictions, val_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looper over Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping the ChannelVIT over Folds\n",
    "\n",
    "# To see the Timing!\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"\\n===== NUMBER OF GPUs {torch.cuda.device_count()} =====\")\n",
    "\n",
    "# Initialize GPU monitoring log file\n",
    "gpu_log_file = \"gpu_usage.log\"\n",
    "with open(gpu_log_file, 'w') as f:\n",
    "    f.write(\"Timestamp,Fold,Epoch,Batch,GPU_Util,Memory_Used,Memory_Total,Temperature\\n\")\n",
    "\n",
    "\n",
    "fold_num = 1\n",
    "for train_index, val_index in kf.split(all_indices):\n",
    "    print(f\"\\n===== Starting Fold {fold_num} =====\")\n",
    "\n",
    "\n",
    "    # Log GPU state at start of fold\n",
    "    fold_start_gpu = get_gpu_usage()\n",
    "    if fold_start_gpu:\n",
    "        print(f\"\\nGPU State at start of Fold {fold_num}:\")\n",
    "        print_gpu_metrics(fold_start_gpu)\n",
    "        log_gpu_metrics(fold_start_gpu, gpu_log_file, epoch=0, batch=0)\n",
    "\n",
    "    # --- Reinitialize the model, classifier, optimizer, and loss function for each fold ---\n",
    "    # Model\n",
    "    model = ChannelVisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        in_chans=in_chans,\n",
    "        embed_dim=embed_dim,\n",
    "        depth=depth,\n",
    "        num_heads=num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "    ).to(device)\n",
    "    # Data Parallel in case to have more than one GPUs (need to re-write the whole code using DDP)\n",
    "    if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    # Classifier\n",
    "    classifier = nn.Linear(embed_dim, 1).to(device)\n",
    "    # Optimizaer\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": model.parameters(), \"lr\": 1e-5},\n",
    "        {\"params\": classifier.parameters(), \"lr\": 1e-3}\n",
    "    ])\n",
    "    # Loss Function\n",
    "    criterion = nn.MSELoss()\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Create train and validation datasets with appropriate augmentation settings\n",
    "    train_dataset = MultiBandGrayTIFFDataset(\n",
    "        metadata_path=metadata_path,\n",
    "        transform_cfg=transform_cfg,\n",
    "        is_train=True,  # Heavy augmentation for training\n",
    "        channels=[0, 1, 2, 3, 4, 5],\n",
    "        channel_mask=False,\n",
    "        indices=train_index.tolist()\n",
    "    )\n",
    "    val_dataset = MultiBandGrayTIFFDataset(\n",
    "        metadata_path=metadata_path,\n",
    "        transform_cfg=transform_cfg,\n",
    "        is_train=False,  # Minimal augmentation for validation\n",
    "        channels=[0, 1, 2, 3, 4, 5],\n",
    "        channel_mask=False,\n",
    "        indices=val_index.tolist()\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Note: model, classifier, optimizer, and criterion remain constant across folds.\n",
    "    r2_history = []\n",
    "    bias_history = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "                # Log GPU state at start of epoch\n",
    "        epoch_start_gpu = get_gpu_usage()\n",
    "        if epoch_start_gpu:\n",
    "            print(f\"\\nGPU State at start of Epoch {epoch}:\")\n",
    "            print_gpu_metrics(epoch_start_gpu)\n",
    "            log_gpu_metrics(epoch_start_gpu, gpu_log_file, epoch=epoch, batch=0)\n",
    "        \n",
    "        train_loss, val_loss, r2, rmse, bias, val_predictions, val_actuals = run_epoch(\n",
    "            fold_num, epoch, train_loader, val_loader, model, classifier, optimizer, criterion, device)\n",
    "        \n",
    "        # Log GPU state after epoch\n",
    "        epoch_end_gpu = get_gpu_usage()\n",
    "        if epoch_end_gpu:\n",
    "            print(f\"\\nGPU State after Epoch {epoch}:\")\n",
    "            print_gpu_metrics(epoch_end_gpu)\n",
    "            log_gpu_metrics(epoch_end_gpu, gpu_log_file, epoch=epoch, batch='end')\n",
    "        \n",
    "        # Run Epochs and Save Predictions\n",
    "        r2_history.append(r2)\n",
    "        bias_history.append(bias)\n",
    "        \n",
    "        # Save predictions, actuals, and compute residuals (Y - Y_hat) for each sample in this epoch\n",
    "        for i, (pred, actual) in enumerate(zip(val_predictions, val_actuals)):\n",
    "            residual = actual - pred  # Y - Y_hat\n",
    "            results_list.append({\n",
    "                \"fold\": fold_num,\n",
    "                \"epoch\": epoch,\n",
    "                \"original_index\": val_index[i],\n",
    "                \"predicted\": pred,\n",
    "                \"actual\": actual,\n",
    "                \"residual\": residual\n",
    "            })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Total Time in the Epoch: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Log GPU state at end of fold\n",
    "    fold_end_gpu = get_gpu_usage()\n",
    "    if fold_end_gpu:\n",
    "        print(f\"\\nGPU State at end of Fold {fold_num}:\")\n",
    "        print_gpu_metrics(fold_end_gpu)\n",
    "        log_gpu_metrics(fold_end_gpu, gpu_log_file, epoch='end', batch='end')\n",
    "    \n",
    "    # Save History\n",
    "    fold_r2_history[f\"Fold_{fold_num}\"] = r2_history\n",
    "    fold_bias_history[f\"Fold_{fold_num}\"] = bias_history\n",
    "    fold_num += 1\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Log final GPU state\n",
    "final_gpu = get_gpu_usage()\n",
    "if final_gpu:\n",
    "    print(\"\\nFinal GPU State:\")\n",
    "    print_gpu_metrics(final_gpu)\n",
    "    log_gpu_metrics(final_gpu, gpu_log_file, epoch='final', batch='final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2 = pd.DataFrame(fold_r2_history)\n",
    "df_r2.to_csv(\"fold_r2_history.csv\", index=False)\n",
    "print(\"Saved fold_r2_history.csv\")\n",
    "\n",
    "df_bias = pd.DataFrame(fold_bias_history)\n",
    "df_bias.to_csv(\"fold_bias_history.csv\", index=False)\n",
    "print(\"Saved fold_bias_history.csv\")\n",
    "\n",
    "\n",
    "# Save Predictions, Actual Values, and Residuals to CSV\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_csv_path = \"predictions_logfiles.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\nPredictions, actual values, and residuals saved to {results_csv_path}\")\n",
    "\n",
    "# Keep Only Final-Epoch Predictions\n",
    "###############################################################################\n",
    "# Filter for epoch=10 (the final epoch)\n",
    "final_epoch_df = results_df[results_df[\"epoch\"] == epochs].copy()\n",
    "\n",
    "# We should have exactly one row per sample (0..3654) because each sample is in exactly one fold's val set.\n",
    "# Sort by original_index\n",
    "final_epoch_df.sort_values(\"original_index\", inplace=True)\n",
    "final_epoch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Keep only the columns [original_index, actual, predicted]\n",
    "final_epoch_df = final_epoch_df[[\"original_index\", \"actual\", \"predicted\"]]\n",
    "\n",
    "# 1-based indexing (like metadata 1, 2, ..., 3655):\n",
    "final_epoch_df[\"original_index\"] += 1\n",
    "\n",
    "# 7) Save to Single CSV with 3655 rows (if you have 3655 samples)\n",
    "final_epoch_df.to_csv(\"final_predictions.csv\", index=False)\n",
    "print(\"Saved final_predictions.csv with one row per original sample.\")\n",
    "\n",
    "print(final_epoch_df.head())\n",
    "print(f\"Total rows in final CSV: {len(final_epoch_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model\n",
    "torch.save(model, \"channelvit_sixbands_full_model.pth\")\n",
    "print(\"Full model saved as channelvit_sixbands_full_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Plot Learning Curves for R² and Bias\n",
    "plt.figure(figsize=(12, 6))\n",
    "for fold, r2_hist in fold_r2_history.items():\n",
    "    plt.plot(range(1, epochs+1), r2_hist, marker='o', label=f\"{fold} R²\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"R² Score\")\n",
    "plt.title(\"R² Score per Epoch for Each Fold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"r2_per_epoch_crossval.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for fold, bias_hist in fold_bias_history.items():\n",
    "    plt.plot(range(1, epochs+1), bias_hist, marker='o', label=f\"{fold} Bias\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Bias (Mean Error)\")\n",
    "plt.title(\"Bias per Epoch for Each Fold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"bias_per_epoch_crossval.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Add Predicted vs. Actual Scatter Plot\n",
    "# -------------------------------------------------------------------\n",
    "actual_values = final_epoch_df[\"actual\"].values\n",
    "predicted_values = final_epoch_df[\"predicted\"].values\n",
    "\n",
    "# Compute R² and RMSE for annotation\n",
    "r2 = r2_score(actual_values, predicted_values)\n",
    "rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(actual_values, predicted_values, alpha=0.5, label=\"Predicted vs. Actual\")\n",
    "\n",
    "# Plot a perfect-fit line\n",
    "min_val, max_val = min(actual_values), max(actual_values)\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', label=\"Perfect Fit\")\n",
    "\n",
    "# Annotate with R² and RMSE\n",
    "plt.text(\n",
    "    0.05, 0.95,\n",
    "    f\"R²: {r2:.3f}\\nRMSE: {rmse:.3f}\",\n",
    "    transform=plt.gca().transAxes,\n",
    "    fontsize=12,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(facecolor='white', alpha=0.6)\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs. Actual (Final Epoch, All Folds)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"predicted_vs_actual.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GPU usage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gpu_metrics(log_file=\"gpu_usage.log\"):\n",
    "    \"\"\"Plot GPU usage metrics from the log file\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(log_file)\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot GPU Utilization\n",
    "        ax1.plot(df['GPU_Util'])\n",
    "        ax1.set_title('GPU Utilization %')\n",
    "        ax1.set_ylabel('Utilization %')\n",
    "        \n",
    "        # Plot Memory Usage\n",
    "        ax2.plot(df['Memory_Used'] / 1024)  # Convert to GB\n",
    "        ax2.set_title('GPU Memory Usage (GB)')\n",
    "        ax2.set_ylabel('Memory (GB)')\n",
    "        \n",
    "        # Plot Temperature\n",
    "        ax3.plot(df['Temperature'])\n",
    "        ax3.set_title('GPU Temperature (°C)')\n",
    "        ax3.set_ylabel('Temperature °C')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('gpu_metrics.pdf')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting GPU metrics: {e}\")\n",
    "\n",
    "# Call this after training is complete\n",
    "plot_gpu_metrics()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
